#+TITLE: Investigating the impact of DCT image compression on the performace of image classification with a convolutional neural network.
How is the percision and speed of an image classification convolutional neural network affected when images are compressed using DCT image compression techniques?
* Introduction
Computer vision is used extensivley in our modern society and to great effect. It has been utilized in numerous fields such as autonomus vehicles and facial recognition. Most of these application utilize convolutional neural networks (CNNs) as they are very effective in image classification. However a limitation CNNs face are the large datasets they require.

In order for computer vision models to become accurate and useful in the real word they must process large amounts of data, a computationaly expensive task. However with our societies transition into an era centered around the Internet-of-Things (IoT) the prevelance of realword data is skyrocketing but the computational power of these IoT devices is not nearly enough to handle the data onsite and so it often will be distributed to an offsite system with the required computiational ability. An effective way to incorporate this would be compressing the data before transferring it as it would allow for less use of bandwidth and potentially less system resources due to the smaller data sizes.

A common method of image compression is using the discrete cosine transform (DCT). There are many different compression types that utilize the discrete cosine transform such as JPEG, HEIF, BPG, etc.. The DCT is used to remove repetitive and unimportant data from an image, therefor decreasing the images size, however this can cause the image to suffer losses in quality if compressed enough.

The objective of this paper is to study how qulaity degredation caused by different types of image compression using the DCT effects the performance, specificaly the classification speed and percision, of the images classification using a CNN.
* Background Research
** CNN
A convolutional neural network (CNN) is a type of deep neural network which is essential in the computer vision field. A CNN takes input images and then identifies features of different aspects of the images to be able to differentiate the images from one another. Because of the prevalence of CNNs in image recognition a CNN will be the algorithm used. A CNN consists mainly of four parts as seen in figure 2.1.1.
#+CAPTION: Figure 2.1.1
 [[file:diagrams/cnn.svg]]
*** Input Image
CNNs take an image as an input and the images are structured similar to figure 2.1.2. They are read as multi-dimensional arrays, and as such when images of much larger resolutions are inputted the process would become much more computationally expensive. This is where the CNN is used as it will reduce the resolution of the image, making it easier to process, without loosing key features needed for differentiating the different images.
#+CAPTION: Figure 2.1.2
[[file:diagrams/cnn-input.svg]]
*** Convolution Layer
A CNN potentially could try and analyze the input image directly however as previously stated when images' resolution increases it would become much more computationally expensive. Even medium sized images would produce huge numbers of input nodes causing training times to be unfeasible.

To solve this problem CNNs instead look for patterns in the structure of images and then extracts them for analysis. As discussed above each image can be represented by a multi-dimensional array or a matrix and in order to extract the patterns another matrix passes over the image matrix. This matrix acts as a filter and is called the "kernel" and there are many of these kernels each one extracting a particular pattern. While the kernel is passing over the image matrix, matrix multiplication is performed on the overlapping areas and then the sum is found for the resulting matrix and inputted into the output matrix, also called the feature map. This process can be seen below in figure 2.1.3.
#+CAPTION: Figure 2.1.3
[[file:diagrams/cnn-kernel.svg]]
The kernel passes over the image matrix in a scrolling motion as show below in figure 2.1.4.
#+CAPTION: Figure 2.1.4
[[file:diagrams/cnn-kernel-movement.svg]]
*** Pooling Layer
The pooling layer is similair to the convolution layer in the sense that its aim is to decrease the amount of data needed to be proccessed. The amount of data is decreased through dimensional reduction. There are multiple methods for pooling like max, average, etc. 

The first step of pooling is defining a "spacial neighborhood", and then applying the desired transformation on that data. For example if max pooling is used then the largest value is taken, and if average pooling is used then the average of the values in the "spacial neighborhood" is taken. This can be seen in figure 2.1.5.
#+CAPTION: Figure 2.1.5
[[file:diagrams/cnn-pooling.svg]]
*** Fully Connected Layer
In order to classify the images a fully connected layer is used. A fully connected layer is a multi-layer perceptron. The input data must be flattened as it is passed to a feed forward neural network and after a series of epochs the model can distinguish features from different types of images and then can classify them with the softmax activation function.
#+CAPTION: Figure 2.1.6
[[file:image path]]
** Discrete Cosine Transform

** Encoders
*** BPG
BPG is a relatively new type of image compression based off of the video compression technique High Efficiency Video Coding (HEVC) standard. HEVC is considered to be a major leap forward in compression techniques due to having double the compression efficiency of traditional compression techniques. BPG takes advantage of all the benefits of HEVC to create a compression type that is more effective but also produces better quality images than traditional forms of compression.
*** HEIF
HEIF is another more recent image compression that is also based off of HEVC. HEIF is developed by MPEG and was adopted by apple in 2017 due to it being container based and able to store bursts of images effectively due to HEIF acting as a container able to store multiple images. 
*** JPEG
JPEG is a very common form of lossy image compression commonly used mainly by the world wide web and digital cameras. JPEG often acheives a compression ratio of 10 with only small perceptible losses in quality. JPEG images are very efficient when compressing images with smooth gradual changes in color, rather than sharp changes. JPEG compression splits an encoded image into 8x8 pixel blocks and then applies a DCT to each block, because of this JPEG compression is notorious for having a "tiling" effect when images are compressed at a high compression ratio.
*** JPEG XR
JPEG XR is a compression type developed by JPEG and was targeted at having an extended dynamic range as it has support for 32 bit images compared to JPEG which supports 16 bit images. The encoding scheme is also more efficient than JPEG allowing for achieving a better compression ratio while still maintaining a visually acceptable level.
*** WebP
WebP is a modern image format aimed at making the World Wide Web faster. It achieves this by by providing superior lossy and lossless compression. WebP lossy image compression is based on VP8, a video compression format, key frame encoding. WebP uses block prediction dividing the frame into smaller sections. Using block prediction WebP detects redundant colors based off of previous blocks, this is called predictive coding. Web also achieves better compression compared to JPEG by using boolean arithmetic encoding. 
** Metrics
*** Compression Ratio
\begin{equation*}
C_R = \frac{n_1}{n_2}
\end{equation*}
*** MSE
\begin{equation*}
MSE = \frac{1}{n}\sum\limits_{i=1}^n (\hat{Y}_i - Y_i)^2
\end{equation*}
*** PSNR
The Peak Signal to Noise Ratio (PSNR) is one of the most commonly used quality metrics when comparing the effects of image compression with the original image. The PSNR expresses the ratio between the maximum value of a signal and the value of distorting noise. It is calculated as follows using the MSE.
\begin{equation*}
PSNR = 20\log_{10}{\left(\frac{MAX_I}{\sqrt{MSE}}\right)}
\end{equation*} 
This means that the higher PSNR the better the accuracy of the image compared to the original. 
*** SSIM
Even though PSNR is a very common metric for evaluating the efficiency of image compression, it is unable to evaluate the image qualities in the same way as humans. The Structural SIMilarity (SSIM) was created because of this as it, unlike PSNR and MSE, perceive image degradation as the change in the images structural information. 
\begin{equation*}
SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}
\end{equation*}

* Experiment Methodology
#+BEGIN_COMMENT
Make more about what I am doing not as much "How"
- I can add more to the top section about exactly what is happening and what I am doing and then discuss the how in the subsections.
#+END_COMMENT
Data gathered experimentaly is the main source of data of this paper. The images were classified using Caffe with the pretrained network bvlc_alexnet due to its prevelance among numerous other papers.
** Validation Dataset
The CNNs performance was measured by using images from the validation dataset of ILSVRC19. Validation data is used as it ensures that when the model was being trained none of the images in the validation data were used.

In order to preprocess the dataset a subset of images from the validation set were randomly gathered and then compressed using various compression algorithims. A subset was chosen from the data as the original ILSVRC19 validation set contains over 50,000 images which is far to many to be analyzed effectivley and so 60 images were chosen at random to be part of the new dataset. A python script was used in order to facilitate the random selection and create a uniform directory structure.

After images were chosen for the new dataset a baseline would be run using the pretrained CNN in order to determine the top-1 and top-5 accuracy. This was used in order to confirm that the dataset selected was repersentative of the whole validation set used in imagenet. It was found that the top-1 accuracy for the subset is *XXX* and top-5 accuracy is *XXX*. *XXX*

After the integretity of the dataset is confirmed the images in the dataset were compressed multiple times at different compression qualities. This was completed by using multiple encoders with different parameters and automating the whole process with a python script.
** Encoder Parameters
All encoder parameters will be run using the default parameters except for the quantization or quality parameter which will be set at 10 different levels on a scale of 0 to 100.
** Metrics
Metrics will be applied to the subset of the validation set once all variations of images are created with the encoders. The metrics will be gathered using ~wand~ and image magick library for python. ~wand~ will accept different formats of images and is able to compare them automatically using a number of metrics. The data gathered about the metrics for the images will then be stored on disk in order to be accessed again.
*ADD EXPLANATION WHY WAND WAS USED OVER OTHER LIBRARIES*
** Variables
*** Accuracy
#+BEGIN_COMMENT
For measuring the accuracy I am still undecided as there are two methodolgies I could follow. 
- Comparing the rank difference. This is good because it is used in a lot of journals but I dont really like it because it is very strange to graph. I also do not understand it to well.
- Measuring the classification accuracy. This is good because it can produce better graphs and I understand it but I do not know if it is a respected way of measuring accuracy.

*NEED MORE RESEARCH*
#+END_COMMENT
Accuracy will be determined using two techniques. An accuracy score metric that measures the mean accuracy of classification and the rank numbers of classifcation. The score metric I believe will give me the oppurtunity to create more useful visualizations but rank numbers will help allow me to observe other paterns of the method of classification.
*** Speed
The speed of the classification was determined by the amount of time the CNN took in order to classify an image after first being persented the image. In order to obtain the time taken the ~time~ python library was used.
** Experimental Procedure
#+CAPTION: Preprocessing Procedure
[[file:diagrams/preprocessing.svg]]
#+CAPTION: Classification Procedure
[[file:diagrams/classification.svg]]
* Experiment Results
** Quantitative Analysis
*** Accuracy
**** BPG
The table below contains the experimental results for each image compressed in the BPG format.
**** HEIF
The table below contains the experimental results for each image compressed in the HEIF format.
**** JPEG
The table below contains the experimental results for each image compressed in the JPEG format.
**** JPEGXR
The table below contains the experimental results for each image compressed in the JPEGXR format.
**** WebP
The table below contains the experimental results for each image compressed in the WebP format.
*** Speed
#+BEGIN_COMMENT
Need to add experimental results about speed.
#+END_COMMENT
** Qualitative Analysis
*** Accuracy
**** BPG
#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/BPG-SSIM-CR.png]]
**** HEIF
#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/HEIF-SSIM-CR.png]]
**** JPEG
#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/JPEG-SSIM-CR.png]]
**** JPEGXR
#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/JPEGXR-SSIM-CR.png]]
**** WebP
#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/WebP-SSIM-CR.png]]
*** Speed
#+BEGIN_COMMENT
Need to add experimental results about speed.
#+END_COMMENT
* Discussion
** Analyze Structures
#+CAPTION: ILSVRC2012_val_00007154 - Figure
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00007154.JPEG]]

#+CAPTION: ILSVRC2012_val_00011022 - Figure 
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00011022.JPEG]]

#+CAPTION: ILSVRC2012_val_00049831 - Figure
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00049831.JPEG]]

** Analyze Birds
#+CAPTION: ILSVRC2012_val_00015392
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00015392.JPEG]]

#+CAPTION: ILSVRC2012_val_00019998
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00019998.JPEG]]

#+CAPTION: 
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00025491.JPEG]]

#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00033756.JPEG]]

#+CAPTION: image caption
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00039374.JPEG]]

** Analyzing Positive Correlation
** Analyzing Negative Correlation
** Analyzing SSIM vs CR
#+CAPTION: 
[[file:/home/jrobertboos/Code/extended-essay/data/graphs/ILSVRC2012_val_00021915.JPEG]]

As repersented in these graphs the SSIM graphs are always the opposite of the CR graphs.




* Conclusion
* Works Cited 
* Appendix
